# 자연어 처리 모델의 성능을 높이는 비결 - 임베딩

### 임베딩?
* 단어나 문장을 벡터로 바꾼 것 혹은 그 과정

word2vec을 사용. 코사인 유사도.

벡터 연산 (유추 평가) 가능

아들 - 딸 + 소녀 = 소년?

전이학습 (transfer learning)
* 초기 정확도 자체가 높음

임베딩이 좋아지면? => 딥러닝 모델의 향상

### 임베딩으로 문서 분류하기
단어 임베딩의 합으로 문서 벡터를 표현<br>
도형의 중심은 평균과 밀접한 관련<br>
단어 벡터의 합 ≒ 단어 벡터의 평균 ≒ 문서 벡터의 중심

복잡한 모델을 써도 80%대<br>
임베딩 품질이 좋으면 자연어 처리 성능을 높일 수 있음


### 임베딩이 어떤 의미를 가지는가
* 문서의 의도는 단어 사용 패턴에 따라 들어난다
* 단어가 어떤 순서로 나타나는지 => 시퀸스 정보에 의미가 녹아있다 => ELMo, BERT
* 단어가 어떤 단어와 주로 같이 나타나는지 => 문맥에 의미가 녹아있다 => Word level embedding

### 문장 수준 임베딩
* ELMo, BERT
* 동음이의어 분간 가능 => 문장의 문맥적 의미를 벡터화

### 임베딩 활용
* 전이학습에서 가장 크게 쓰일 수 있음

https://github.com/ratsgo/embedding<br>
https://github.com/SKTBrain/KoBERT

### Q&A
* ELMo는 임베딩이 떨어지는 문제는 없음
